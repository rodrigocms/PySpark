# Databricks notebook source
# MAGIC %md
# MAGIC ## Power BI API - Workspaces, Datasets and Data Sources 
# MAGIC 
# MAGIC Notebook to Work as a start point on Start Datasets Data Sources development.
# MAGIC 
# MAGIC Documentation: https://docs.microsoft.com/en-us/rest/api/power-bi/datasets/get-datasources-in-group

# COMMAND ----------

# MAGIC %run /Users/rsouza1@emea.corpdir.net/API/Power_BI_API_Functions

# COMMAND ----------

import requests
import msal
import json
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime, timedelta, date


# COMMAND ----------

# DBTITLE 1,Get all Workspaces and write to table
df_workspaces = pbiApi_get_df_info_all_workspaces()

df_workspaces = (df_workspaces
                      .withColumn("partition_date",date_format(current_date(),"yyyy-MM-dd"))  
                      .withColumn("ApiUpdate_date",date_format(current_date(),"yyyy-MM-dd"))
                      .withColumn("ApiUpdate_timestamp",to_timestamp(current_timestamp(),"yyyy-MM-dd HH mm ss SSS")))

#display(df_workspaces)

# COMMAND ----------

#Write to Delta Table 'analystsdb.onedna_pbiapi_workspaces'
replace_condition = "partition_date = '{}'".format(datetime.today().strftime('%Y-%m-%d'))

df_workspaces.cache()

(df_workspaces.write 
        .format("delta") 
        .mode("overwrite") 
        .option("overwriteSchema", "true")
        .option("replaceWhere",replace_condition )
        .partitionBy('partition_date')
        .saveAsTable('analystsdb.onedna_pbiapi_workspaces')
) 

# COMMAND ----------

# DBTITLE 1,Get All Datasets and Write to Table
df_datasets = pbiApi_get_df_info_all_datasets_in_workspaces()

df_wsp_name = df_workspaces.orderBy('partition_date',asc=False).select('workspace_id','workspace_name').dropDuplicates()

#add workspace name to dataframe info
df_ds = df_datasets.join(df_wsp_name, df_datasets.workspaceId == df_wsp_name.workspace_id )
df_ds = df_ds.select('workspace_name', 'dataset_name', 'dataset_isRefreshable', 'dataset_configuredBy', 'dataset_targetStorageMode', 'workspace_id', 'dataset_id', 'dataset_createdDate')

#add partition date
df_ds = (df_ds.withColumn("partition_date",date_format(current_date(),"yyyy-MM-dd"))  
                      .withColumn("ApiUpdate_date",date_format(current_date(),"yyyy-MM-dd"))
                      .withColumn("ApiUpdate_timestamp",to_timestamp(current_timestamp(),"yyyy-MM-dd HH mm ss SSS")))
#display(df_ds)

# COMMAND ----------

#Write to Delta Table 'analystsdb.onedna_pbiapi_datasets'

df_ds.cache()

(df_ds.write 
        .format("delta") 
        .mode("overwrite") 
        .option("overwriteSchema", "true")
        .saveAsTable('analystsdb.onedna_pbiapi_datasets')
) 

# COMMAND ----------

# DBTITLE 1,Dataframe - Data Sources
### Getting Data Sources Info from API 

Schema= StructType([
                    StructField('workspaceId',StringType(), False),
                    StructField('datasetId',StringType(), False),
                    StructField('source_datasourceId',StringType(), False),
                    StructField('source_datasourceType',StringType(), False),
                    StructField('source_connectionDetails',StringType(), False),
                    StructField('source_gatewayId',StringType(), False)
                    ])

df_sources = spark.createDataFrame(data = [],
                           schema = Schema)
lst=[]

#get the list of all available dataset (to the Service Principal)
list_datasets = pbiApi_get_list_all_workspaces_datasets()

# create a df with the data source information from all datasets
for w_ds in list_datasets:

  workspaceId = w_ds[0]
  datasetId = w_ds[1]

  lst=[]
  URL = f'https://api.powerbi.com/v1.0/myorg/groups/{workspaceId}/datasets/{datasetId}/datasources'
  HEADERS = pbiApi_get_token_header()[1]
  api_out_r = requests.get(url=URL, headers=HEADERS )
  reportsJson = api_out_r.json()
  s=0

  if api_out_r.status_code == 404:
    source_datasourceId = reportsJson['error']['code']
    source_datasourceType = 'ItemNotFound'
    source_connectionDetails = 'ItemNotFound'
    source_gatewayId = 'ItemNotFound'
  elif (api_out_r.status_code == 200) & (len(reportsJson['value'])==0) :
    source_datasourceId = ''
    source_datasourceType = ''
    source_connectionDetails = ''
    source_gatewayId = ''
  elif api_out_r.status_code == 200:
    source_datasourceId = reportsJson['value'][s]['datasourceId']
    source_datasourceType = reportsJson['value'][s]['datasourceType']
    source_connectionDetails = str(reportsJson['value'][s]['connectionDetails'])
    source_gatewayId = reportsJson['value'][s]['gatewayId']   
  info_list = [workspaceId, datasetId, source_datasourceId, source_datasourceType, source_connectionDetails, source_gatewayId]
  lst.append(info_list)
  
  df = spark.createDataFrame(lst, ['workspaceId', 'datasetId', 'source_datasourceId', 'source_datasourceType', 'source_connectionDetails', 'source_gatewayId'])
  df_sources = df_sources.union(df)

#display(df_sources)

# COMMAND ----------

### Adding Columns to DF Datasources
df_sources = (df_sources
                      .withColumn("partition_date",date_format(current_date(),"yyyy-MM-dd"))  
                      .withColumn("ApiUpdate_date",date_format(current_date(),"yyyy-MM-dd"))
                      .withColumn("ApiUpdate_timestamp",to_timestamp(current_timestamp(),"yyyy-MM-dd HH mm ss SSS"))
                      .withColumn('Path', regexp_extract(col('source_connectionDetails'),"(path\': \')(.*)(\')",2))
                      .withColumn('Connection_Kind', regexp_extract(col('source_connectionDetails'),"('kind': ')(.*)('})",2))
                      .withColumn('source_datasource_Type', when(col('Connection_Kind')=='Databricks','Databricks').otherwise(col('source_datasourceType')))
                      .withColumn('Connection_Host', regexp_extract(col('source_connectionDetails'),'''(\{\"host":")(.*)(",)''',2))
                      .withColumn('Connection_httpPath', regexp_extract(col('source_connectionDetails'),'''(httpPath":")(.*)(\"})''',2))
                      .withColumn('Connection_File', when(col('source_datasourceType')=='File', col('Path') ))
                      .withColumn('Connection_AzBlob_Account', regexp_extract(col('source_connectionDetails'),'''(\{'account': ')(.*)(', 'domain': ')(.*)('})''',2))
                      .withColumn('Connection_AzBlob_Domain', regexp_extract(col('source_connectionDetails'),'''(\{'account': ')(.*)(', 'domain': ')(.*)('})''',4))
                      .withColumn('Link_DatasetDetails', concat(lit('https://app.powerbi.com/groups/'),col('workspaceId'),lit('/datasets/'),col('DatasetId'),lit('/details')))
             )

#Select columns
df_sources = df_sources.select('source_datasource_Type', 'source_connectionDetails' ,'Connection_Kind' , 'Connection_Host' , 'Connection_httpPath' , 'Connection_File' , 'Connection_AzBlob_Account' , 'Connection_AzBlob_Domain' , 'Link_DatasetDetails', 'workspaceId' , 'datasetId' ,  'source_datasourceId' , 'source_gatewayId' , 'ApiUpdate_date' , 'ApiUpdate_timestamp','partition_date'    )

# COMMAND ----------

display(df_sources)

# COMMAND ----------

dataset_name = df_ds.orderBy('partition_date',asc=False).select('workspace_name','dataset_name','dataset_id').dropDuplicates()
#display(dataset_name)

df_s = df_sources.join(dataset_name, df_sources.datasetId == dataset_name.dataset_id)
df_s = df_s.select( 'workspace_name', 'dataset_name', 'Connection_Kind', 'Connection_Host', 'Connection_httpPath',   'source_datasource_Type', 'source_connectionDetails', 'Connection_File', 'Connection_AzBlob_Account', 'Connection_AzBlob_Domain', 'Link_DatasetDetails', 'workspaceId', 'datasetId', 'source_datasourceId', 'source_gatewayId', 'partition_date', 'ApiUpdate_date', 'ApiUpdate_timestamp')
display(df_s)

# COMMAND ----------

#Write to Delta Table 'analystsdb.onedna_pbiapi_datasources'
replace_condition = "partition_date = '{}'".format(datetime.today().strftime('%Y-%m-%d'))

(df_s.write 
        .format("delta") 
        .mode("overwrite") 
        .option("overwriteSchema", "true")
        .option("replaceWhere",replace_condition )
        .partitionBy('partition_date')
        .saveAsTable('db_target.table_target')
) 
