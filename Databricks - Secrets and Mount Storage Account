
### GET SECRETS AND MOUNT STOREGE ACCOUNTS TO DATABRICKS

#list key voult scopes
dbutils.secrets.listScopes()

#list key voult scopes - specific scope
dbutils.secrets.listScopes("demo")

#get secret
dbutils.secrets.get(scope="demo",key="storageread")


## MOUNT DATA CONTAINER TO DATABRICKS

#unmount directory if previusly mounted
MOUNTPOINT = 'mnt/commonfiles'
if MOUNTPOINT in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:
	dbutils.fs.unmount(MOUNTPOINT)
	
#Add the Storage Account Container, and reference the secrets to pass the SAS token
STORAGE_ACCOUNT = dbutils.secrets.get(scope="demo", key="storageaccount")
CONTAINER = 'commonfiles'
SASTOKEN = dbutils.secrets.get(scope="demo", key="storagearead")

#Do not change these values
SOURCE = 'wasbs://{container}@{storage_acct}.blob.core.windows.net/'.format{container=CONTAINER,storage_acct=STORAGE_ACCOUNT}
URI = 'fs.azure.sas.{container}.{storage_acct}.blob.core.windows.net/'.format{container=CONTAINER,storage_acct=STORAGE_ACCOUNT}

try :
	dbutils.fs.mount(
		source=SOURCE,
		mount_point=MOUNTPOINT,
		extra_configs={URI:SASTOKEN})
except Exception as e:
	if "Directory already mounted" in str(e):
		pass #Ignore error if already mounted
	else:
		raise e 
		
display(dbutils.fs.ls(MOUNTPOINT))

---------------------- end mount

df = (spark.read.parquet(MOUNTPOINT + 'sales.parquet')

------ 

#unmount MOUNTPOINT after the job is done - Data will be accessable to any user in the workspace while source is mounted

if MOUNTPOINT in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:
	dbutils.fs.unmount(MOUNTPOINT)


